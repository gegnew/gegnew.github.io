---
layout: post
title: Transitioning to Spark Structured Streaming
categories: [Apache Spark, Structured Streaming]
---

At Relayr, we use Apache Spark and Kafka for most of our anomaly detection
pipeline. Currently, we are not able to reliably process out-of-order data in
our streaming preprocessing pipeline. The current (old) Spark Streaming
appreach does not have tools to ensure that the data is ordered correctly
between shuffles. This can make anomaly detection flaky and cause data loss.

# Building the POC

We determined that the Spark Structured Streaming (SSS) approach is a feasible
solution to this issue (see [aynroot's
POC](https://github.com/aynroot/spark-streaming-poc)). However, we don't want
to replae the entire preprocessing cluster with a structured streaming cluster.
Rather, we will only replace the section that does initial pre-computation.

Our general setup:
    1. Read "device events" from csv or from a socket
    1. Enhance measurements with "aggregation type", "coarse grain step", and "device group"
    1. Split into device-group based datasets, then perform coarse-graining and aggregation.
    1. Each dataset has its own watermark and emits when the watermark window closes
    1. Create a vector.

We needed to ensure that this proof-of-concept could create "sub-datasets" per
device group that each have their own watermarking strategy.

Now we have to transition aynroot's POC into a full pre-processing pipeline. I
don't really know anything about SSS, so I intend to learn more and document it
here.

# About Spark Structured Streaming

SSS is built around the idea of a [Continuous
Application](https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html), 
"an end-to-end application that reacts to data in real-time."

![](/images/continuous-apps.png)

[This
article](https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html)
is a good explanation of structured streaming.

SSS is intended to address three issues (all of which we currently struggle with):
    1. Consistency
    1. Fault tolerance
    1. Out-of-order data

Typically, these issues are left to the user to resolve, which often results in
quite complicated code. SSS promises that "at any time, the output of the
application is equivalent to executing a batch job on a prefix of the data".
From a developer perspective, this is potentially a lifesaver. I find it much
easier to think about batch-processing of data, which often leaves me mucking
around downstream, trying to validate my data. Instead, with SSS, I can write
my pipeline as a batch job and be confident that it will work as a stream.

# Testing it out

